{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e01e6e4",
   "metadata": {},
   "source": [
    "# OpenVINO Model Server in OpenShift demo\n",
    "\n",
    "This notebook demonstrate how to deploy and use OpenVINO Model Server.\n",
    "That will include the use case with BERT model and a pipeline performing face detection operation and also age, gender and emotion recognition for each detected face.\n",
    "\n",
    "Requirements:\n",
    "- OpenShift cluster with the API access to a project. In the demo `ovms` project is used.\n",
    "- installed [OpenVINO Model Server Operator](https://catalog.redhat.com/software/operators/search?q=openvino)\n",
    "- Jupyter session with python3 deployed in the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faeefe5",
   "metadata": {},
   "source": [
    "## Creating Minio storage\n",
    "\n",
    "OpenVINO Model Server can expose over gRPC and REST interface the models stored in the local or cloud storage like AWS S3, google storage or Azure blobs. In OpenShift and Kubernetes every Persistent Storage Claim could be used as well. In this demo will be employed Minio service which is an equivalent of AWS S3.\n",
    "\n",
    "First login to OpenShift cluster API using `oc` tool. In the commands below change the cluster DNS name and the user token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0248c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into \"https://api.openvino5.3q12.p1.openshiftapps.com:6443\" as \"dtrawins\" using the token provided.\n",
      "\n",
      "You have access to 100 projects, the list has been suppressed. You can list all projects with 'oc projects'\n",
      "\n",
      "Using project \"ovms\".\n"
     ]
    }
   ],
   "source": [
    "!oc login --token=<user token> --server=https://api.<cluster dns name>:6443"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ade52f",
   "metadata": {},
   "source": [
    "Change the project context where you would like to deploy your services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b8909a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already on project \"ovms\" on server \"https://api.openvino5.3q12.p1.openshiftapps.com:6443\".\n"
     ]
    }
   ],
   "source": [
    "!oc project ovms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027f09a",
   "metadata": {},
   "source": [
    "Now deploy Minio service. Note that the configuration below creates Minio server with emphemeral storage which will be deleted each time the pod is restarted. It includes also the default credentials. All in all, it is only a demonstrative purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6d7a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/minio created\n",
      "service/minio-service created\n"
     ]
    }
   ],
   "source": [
    "!oc apply -f https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/ovms-demo/notebooks/202-model-server/minio.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d732d",
   "metadata": {},
   "source": [
    "Next step is to connect to the Minio service and create models repository for the OpenVINO Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ce7c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-22 20:49:32--  https://dl.min.io/client/mc/release/linux-amd64/mc\n",
      "Resolving dl.min.io (dl.min.io)... 178.128.69.202\n",
      "Connecting to dl.min.io (dl.min.io)|178.128.69.202|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20529152 (20M) [application/octet-stream]\n",
      "Saving to: ‘mc’\n",
      "\n",
      "mc                  100%[===================>]  19.58M  15.0MB/s    in 1.3s    \n",
      "\n",
      "2021-04-22 20:49:34 (15.0 MB/s) - ‘mc’ saved [20529152/20529152]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.min.io/client/mc/release/linux-amd64/mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ca769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb4845",
   "metadata": {},
   "source": [
    "In the command below make sure you have the correct project name. Replace `ovms` with your project name, where minio got deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deecb396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32mAdded `minio` successfully.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!./mc alias set minio http://minio-service.ovms:9000 minio minio123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9100f8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32;1mBucket created successfully `minio/models`.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!./mc mb minio/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd5ea1",
   "metadata": {},
   "source": [
    "## Creating models repository\n",
    "\n",
    "While the Minio is available, we can upload the models for serving in the OpenVINO Model Server. In the demos below will be needed 4 models:\n",
    "- [resnet](https://github.com/onnx/models/tree/master/vision/classification/resnet)\n",
    "- [face detection](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/face-detection-retail-0004/description/face-detection-retail-0004.md)\n",
    "- [age-gender recognition](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/age-gender-recognition-retail-0013/description/age-gender-recognition-retail-0013.md)\n",
    "- [emotion recognition](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/emotions-recognition-retail-0003/description/emotions-recognition-retail-0003.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b390b316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 30901  100 30901    0     0  48132      0 --:--:-- --:--:-- --:--:-- 48132\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8351k  100 8351k    0     0  8318k      0  0:00:01  0:00:01 --:--:-- 8318k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  101k  100  101k    0     0   969k      0 --:--:-- --:--:-- --:--:--  969k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2297k  100 2297k    0     0  2927k      0 --:--:-- --:--:-- --:--:-- 2927k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 39391  100 39391    0     0   315k      0 --:--:-- --:--:-- --:--:--  315k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 9697k  100 9697k    0     0  10.0M      0 --:--:-- --:--:-- --:--:-- 10.1M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   187  100   187    0     0   1798      0 --:--:-- --:--:-- --:--:--  1780\n",
      "100 97.7M  100 97.7M    0     0  36.6M      0  0:00:02  0:00:02 --:--:--  307M\n"
     ]
    }
   ],
   "source": [
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.xml -o age-gender/1/age-gender-recognition-retail-0013.xml \n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.bin -o age-gender/1/age-gender-recognition-retail-0013.bin\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/face-detection-retail-0004/FP32/face-detection-retail-0004.xml -o face-detection/1/face-detection-retail-0004.xml\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/face-detection-retail-0004/FP32/face-detection-retail-0004.bin -o face-detection/1/face-detection-retail-0004.bin\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.xml -o emotions/1/emotions-recognition-retail-0003.xml\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.bin -o emotions/1/emotions-recognition-retail-0003.bin\n",
    "!curl -L --create-dir https://github.com/onnx/models/raw/master/vision/classification/resnet/model/resnet50-caffe2-v1-9.onnx -o resnet/1/resnet50-caffe2-v1-9.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221d4954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...v1-9.onnx:  97.74 MiB / 97.74 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 580.17 MiB/s 0s\u001b[0m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "!./mc cp --recursive age-gender minio/models/\n",
    "!./mc cp --recursive face-detection minio/models/\n",
    "!./mc cp --recursive emotions minio/models/\n",
    "!./mc cp --recursive resnet minio/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b4f192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32m[2021-04-22 20:50:28 UTC]\u001b[0m\u001b[33m 8.2MiB\u001b[0m\u001b[1m age-gender/1/age-gender-recognition-retail-0013.bin\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-04-22 20:50:28 UTC]\u001b[0m\u001b[33m  30KiB\u001b[0m\u001b[1m age-gender/1/age-gender-recognition-retail-0013.xml\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-04-22 20:50:29 UTC]\u001b[0m\u001b[33m 9.5MiB\u001b[0m\u001b[1m emotions/1/emotions-recognition-retail-0003.bin\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-04-22 20:50:29 UTC]\u001b[0m\u001b[33m  38KiB\u001b[0m\u001b[1m emotions/1/emotions-recognition-retail-0003.xml\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-04-22 20:50:28 UTC]\u001b[0m\u001b[33m 2.2MiB\u001b[0m\u001b[1m face-detection/1/face-detection-retail-0004.bin\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-04-22 20:50:28 UTC]\u001b[0m\u001b[33m 102KiB\u001b[0m\u001b[1m face-detection/1/face-detection-retail-0004.xml\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-04-22 20:50:30 UTC]\u001b[0m\u001b[33m  98MiB\u001b[0m\u001b[1m resnet/1/resnet50-caffe2-v1-9.onnx\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!./mc ls -r minio/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a71f26",
   "metadata": {},
   "source": [
    "With the model repository created, we can move on the deploying OpenVINO Model Server in the cluster.\n",
    "\n",
    "## OpenVINO Model Server deployment with a single model\n",
    "\n",
    "The first scenario will be with a serving a single model. In the demo, there will be performed image classification using ResNet50 model in ONNX format.\n",
    "\n",
    "While the operator in place, starting the inference service is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86657700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: intel.com/v1alpha1\n",
      "kind: Ovms\n",
      "metadata:\n",
      "  name: ovms-resnet\n",
      "spec:\n",
      "  aws_access_key_id: \"minio\"\n",
      "  aws_region: \"us-east-1\"\n",
      "  aws_secret_access_key: \"minio123\"\n",
      "  grpc_port: 8080\n",
      "  image_name: registry.connect.redhat.com/intel/openvino-model-server:latest\n",
      "  log_level: INFO\n",
      "  model_name: \"resnet\"\n",
      "  model_path: \"s3://minio-service:9000/models/resnet\"\n",
      "  plugin_config: '{\\\"CPU_THROUGHPUT_STREAMS\\\":\\\"1\\\"}'\n",
      "  replicas: 1\n",
      "  resources:\n",
      "    limits:\n",
      "      cpu: 4\n",
      "      memory: 500Mi\n",
      "  rest_port: 8081\n",
      "  service_type: ClusterIP\n"
     ]
    }
   ],
   "source": [
    "!curl -s https://raw.githubusercontent.com/dtrawins/openvino_notebooks/ovms-demo/notebooks/202-model-server/ovms-resnet.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd3c3e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ovms.intel.com/ovms-resnet created\n"
     ]
    }
   ],
   "source": [
    "!oc apply -f https://raw.githubusercontent.com/dtrawins/openvino_notebooks/ovms-demo/notebooks/202-model-server/ovms-resnet.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c7fa56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           READY     STATUS    RESTARTS   AGE\n",
      "minio-5c57f888dd-69dzq         1/1       Running   0          4m42s\n",
      "ovms-resnet-7cdb696f7b-pxl7z   1/1       Running   0          32s\n",
      "NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\n",
      "minio-service   ClusterIP   172.30.157.153   <none>        9000/TCP            4m43s\n",
      "ovms-resnet     ClusterIP   172.30.88.10     <none>        8080/TCP,8081/TCP   34s\n"
     ]
    }
   ],
   "source": [
    "!oc get pod\n",
    "!oc get service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f7d32",
   "metadata": {},
   "source": [
    "With those steps, OpenVINO Model Server is running and is ready to accept inference requests. The status of models can be queries with a simple REST API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "739c805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"OK\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl http://ovms-resnet.ovms.svc:8081/v1/models/resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74365e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"modelSpec\": {\n",
      "  \"name\": \"resnet\",\n",
      "  \"signatureName\": \"\",\n",
      "  \"version\": \"1\"\n",
      " },\n",
      " \"metadata\": {\n",
      "  \"signature_def\": {\n",
      "   \"@type\": \"type.googleapis.com/tensorflow.serving.SignatureDefMap\",\n",
      "   \"signatureDef\": {\n",
      "    \"serving_default\": {\n",
      "     \"inputs\": {\n",
      "      \"gpu_0/data_0\": {\n",
      "       \"dtype\": \"DT_FLOAT\",\n",
      "       \"tensorShape\": {\n",
      "        \"dim\": [\n",
      "         {\n",
      "          \"size\": \"1\",\n",
      "          \"name\": \"\"\n",
      "         },\n",
      "         {\n",
      "          \"size\": \"3\",\n",
      "          \"name\": \"\"\n",
      "         },\n",
      "         {\n",
      "          \"size\": \"224\",\n",
      "          \"name\": \"\"\n",
      "         },\n",
      "         {\n",
      "          \"size\": \"224\",\n",
      "          \"name\": \"\"\n",
      "         }\n",
      "        ],\n",
      "        \"unknownRank\": false\n",
      "       },\n",
      "       \"name\": \"gpu_0/data_0\"\n",
      "      }\n",
      "     },\n",
      "     \"outputs\": {\n",
      "      \"gpu_0/softmax_1\": {\n",
      "       \"dtype\": \"DT_FLOAT\",\n",
      "       \"tensorShape\": {\n",
      "        \"dim\": [\n",
      "         {\n",
      "          \"size\": \"1\",\n",
      "          \"name\": \"\"\n",
      "         },\n",
      "         {\n",
      "          \"size\": \"1000\",\n",
      "          \"name\": \"\"\n",
      "         }\n",
      "        ],\n",
      "        \"unknownRank\": false\n",
      "       },\n",
      "       \"name\": \"gpu_0/softmax_1\"\n",
      "      }\n",
      "     },\n",
      "     \"methodName\": \"\"\n",
      "    }\n",
      "   }\n",
      "  }\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl http://ovms-resnet.ovms.svc:8081/v1/models/resnet/metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b49834",
   "metadata": {},
   "source": [
    "## Running predition requests\n",
    "\n",
    "Lets copy an axilary file with ImageNet class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "310848e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/openvinotoolkit/model_server/main/example_client/classes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5209b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import numpy as np\n",
    "import classes\n",
    "from tensorflow import make_tensor_proto, make_ndarray, make_tensor_proto\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887924d",
   "metadata": {},
   "source": [
    "Next we create functions formating the array out of input image. The array will be transofrmed to required format and data range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "734a0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/openvinotoolkit/model_server/raw/main/example_client/images/bee.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e74458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img_data):\n",
    "    mean_vec = np.array([0.485, 0.456, 0.406])\n",
    "    stddev_vec = np.array([0.229, 0.224, 0.225])\n",
    "    norm_img_data = np.zeros(img_data.shape).astype('float32')\n",
    "    for i in range(img_data.shape[0]):\n",
    "         # for each pixel in each channel, divide the value by 255 to get value between [0, 1] and then normalize\n",
    "        norm_img_data[i,:,:] = (img_data[i,:,:]/255 - mean_vec[i]) / stddev_vec[i]\n",
    "    return norm_img_data\n",
    "\n",
    "def getJpeg(path, size):\n",
    "    with open(path, mode='rb') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    img = np.frombuffer(content, dtype=np.uint8)\n",
    "    img = cv2.imdecode(img, cv2.IMREAD_COLOR)  # BGR format\n",
    "    # format of data is HWC\n",
    "    # add image preprocessing if needed by the model\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.astype('float32')\n",
    "    #convert to RGB instead of BGR if required by model\n",
    "    #img = img[:, :, [2, 1, 0]]\n",
    "    #convert to NHWC\n",
    "    img = img.transpose(2,0,1)\n",
    "    # normalize to adjust to model training dataset\n",
    "    img = preprocess(img)\n",
    "    img = img.reshape(1,3,size,size)\n",
    "    print(path, img.shape, \"; data range:\",np.amin(img),\":\",np.amax(img))\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e4040",
   "metadata": {},
   "source": [
    "Let's try to classify this image:\n",
    "\n",
    "![image](https://github.com/openvinotoolkit/model_server/raw/main/example_client/images/bee.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f992d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bee.jpeg (1, 3, 224, 224) ; data range: -2.117904 : 2.64\n",
      "Class is with highest score: 309\n",
      "Detected class name: bee\n"
     ]
    }
   ],
   "source": [
    "img1 = getJpeg('bee.jpeg', 224)\n",
    "\n",
    "channel = grpc.insecure_channel(\"ovms-resnet.ovms.svc:8080\")\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = \"resnet\"\n",
    "request.inputs[\"gpu_0/data_0\"].CopyFrom(make_tensor_proto(img1, shape=(img1.shape)))\n",
    "result = stub.Predict(request, 10.0) # result includes a dictionary with all model outputs\n",
    "\n",
    "output = make_ndarray(result.outputs[\"gpu_0/softmax_1\"])\n",
    "ma = np.argmax(output)\n",
    "print(\"Class is with highest score: {}\".format(ma))\n",
    "print(\"Detected class name: {}\".format(classes.imagenet_classes[ma]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ec9b6",
   "metadata": {},
   "source": [
    "That concludes the demonstration of a simple inference request. In the following example, more advance use case will be presented.\n",
    "\n",
    "## Deploying OpenVINO Model Server with a pipeline of models and a custom node implementation\n",
    "\n",
    "In that part of the demo, we will configure, deploy and use the OpenVINO Model Server with a pipeline of models and a custom node.\n",
    "![graph](https://raw.githubusercontent.com/openvinotoolkit/model_server/develop/docs/faces_analysis_graph.svg)\n",
    "\n",
    "![graph](https://github.com/openvinotoolkit/model_server/raw/develop/docs/faces_analysis.png)\n",
    "\n",
    "The models were uploaded to Minio earlier, so now the missing component is the custom node. Face detection model returns a set of 200 detection results. It includes the coordinate and the detection scores. To employ face analysis models we neeed to crop the detected faces, filter the results with low scores and resize the retreived face images to a correct target size.\n",
    "\n",
    "The the functionality or custom node in OVMS Directed Acyclic Graph scheduler, everyone can create arbitrary implementation of the data transformation node in the pipeline. It will be attached to the model server as a dynamic library. Lets use such [example of the custom node](https://github.com/openvinotoolkit/model_server/tree/develop/src/custom_nodes/model_zoo_intel_object_detection).\n",
    "\n",
    "That custom node library could be build with a simple `make` command and docker image. Here we will repeat the commands from the makefile and the [dockerfile](https://github.com/openvinotoolkit/model_server/blob/develop/src/custom_nodes/model_zoo_intel_object_detection/Dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfb6e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'model_server'...\n",
      "remote: Enumerating objects: 585, done.\u001b[K\n",
      "remote: Counting objects: 100% (585/585), done.\u001b[K\n",
      "remote: Compressing objects: 100% (503/503), done.\u001b[K\n",
      "remote: Total 585 (delta 155), reused 275 (delta 62), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (585/585), 4.42 MiB | 39.72 MiB/s, done.\n",
      "Resolving deltas: 100% (155/155), done.\n",
      "tar: .: Cannot utime: Operation not permitted\n",
      "tar: .: Cannot change mode to rwxr-sr-x: Operation not permitted\n",
      "tar: Exiting with failure status due to previous errors\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth=1 -b develop https://github.com/openvinotoolkit/model_server\n",
    "!curl -s https://download.01.org/opencv/master/openvinotoolkit/thirdparty/linux/opencv/opencv_4.5.1-044_centos7.txz | tar --use-compress-program=xz -xf -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd06261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -c -std=c++17 model_server/src/custom_nodes/model_zoo_intel_object_detection/model_zoo_intel_object_detection.cpp -fpic  -I./opencv/include/ -Wall -Wno-unknown-pragmas -Werror -fno-strict-overflow -fno-delete-null-pointer-checks -fwrapv -fstack-protector\n",
    "!g++ -shared -o libcustom_node.so model_zoo_intel_object_detection.o -L./opencv/lib/ -I./opencv/include/ -lopencv_core -lopencv_imgproc -lopencv_imgcodecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a607cb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x. 1 1000890000 1000890000 212104 Apr 22 20:55 libcustom_node.so\n"
     ]
    }
   ],
   "source": [
    "!ls -l libcustom_node.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d8ea1",
   "metadata": {},
   "source": [
    "With the commands above, we created the library `libcustom_node.so`.\n",
    "\n",
    "We will need also the configuration file which defines the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45df8806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model_config_list\": [\n",
      "    {\"config\": {\n",
      "      \"name\": \"face_detection\",\n",
      "      \"base_path\": \"s3://models/face-detection/\",\n",
      "      \"shape\": \"(1,3,400,600)\"}},\n",
      "    {\"config\": {\n",
      "      \"name\": \"age_gender_recognition\",\n",
      "      \"base_path\": \"s3://models/age-gender/\",\n",
      "      \"shape\": \"(1,3,64,64)\"}},\n",
      "    {\"config\": {\n",
      "      \"name\": \"emotion_recognition\",\n",
      "      \"base_path\": \"s3://models/emotions/\",\n",
      "      \"shape\": \"(1,3,64,64)\"}}\n",
      "  ],\n",
      "  \"custom_node_library_config_list\": [\n",
      "    {\"name\": \"object_detection_image_extractor\",\n",
      "      \"base_path\": \"/config/libcustom_node.so\"}\n",
      "  ],\n",
      "  \"pipeline_config_list\": [\n",
      "    {\n",
      "      \"name\": \"find_face_images\",\n",
      "      \"inputs\": [\n",
      "        \"image\"\n",
      "      ],\n",
      "      \"nodes\": [\n",
      "        {\n",
      "          \"name\": \"face_detection_node\",\n",
      "          \"model_name\": \"face_detection\",\n",
      "          \"type\": \"DL model\",\n",
      "          \"inputs\": [\n",
      "            {\"data\": {\n",
      "              \"node_name\": \"request\",\n",
      "              \"data_item\": \"image\"}}],\n",
      "          \"outputs\": [\n",
      "            {\"data_item\": \"detection_out\",\n",
      "              \"alias\": \"detection\"}]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"extract_node\",\n",
      "          \"library_name\": \"object_detection_image_extractor\",\n",
      "          \"type\": \"custom\",\n",
      "          \"demultiply_count\": 0,\n",
      "          \"params\": {\n",
      "            \"original_image_width\": \"600\",\n",
      "            \"original_image_height\": \"400\",\n",
      "            \"target_image_width\": \"64\",\n",
      "            \"target_image_height\": \"64\",\n",
      "            \"convert_to_gray_scale\": \"false\",\n",
      "            \"max_output_batch\": \"100\",\n",
      "            \"confidence_threshold\": \"0.7\",\n",
      "            \"debug\": \"true\"\n",
      "          },\n",
      "          \"inputs\": [\n",
      "            {\"image\": {\n",
      "              \"node_name\": \"request\",\n",
      "              \"data_item\": \"image\"}},\n",
      "            {\"detection\": {\n",
      "              \"node_name\": \"face_detection_node\",\n",
      "              \"data_item\": \"detection\"}}],\n",
      "          \"outputs\": [\n",
      "            {\"data_item\": \"images\",\n",
      "              \"alias\": \"face_images\"},\n",
      "            {\"data_item\": \"coordinates\",\n",
      "              \"alias\": \"face_coordinates\"},\n",
      "            {\"data_item\": \"confidences\",\n",
      "              \"alias\": \"confidence_levels\"}]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"age_gender_recognition_node\",\n",
      "          \"model_name\": \"age_gender_recognition\",\n",
      "          \"type\": \"DL model\",\n",
      "          \"inputs\": [\n",
      "            {\"data\": {\n",
      "              \"node_name\": \"extract_node\",\n",
      "              \"data_item\": \"face_images\"}}],\n",
      "          \"outputs\": [\n",
      "            {\"data_item\": \"age_conv3\",\n",
      "              \"alias\": \"age\"},\n",
      "            {\"data_item\": \"prob\",\n",
      "              \"alias\": \"gender\"}]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"emotion_recognition_node\",\n",
      "          \"model_name\": \"emotion_recognition\",\n",
      "          \"type\": \"DL model\",\n",
      "          \"inputs\": [\n",
      "            {\"data\": {\n",
      "              \"node_name\": \"extract_node\",\n",
      "              \"data_item\": \"face_images\"}}],\n",
      "          \"outputs\": [\n",
      "            {\"data_item\": \"prob_emotion\",\n",
      "              \"alias\": \"emotion\"}]\n",
      "        }\n",
      "      ],\n",
      "      \"outputs\": [\n",
      "        {\"face_images\": {\n",
      "          \"node_name\": \"extract_node\",\n",
      "          \"data_item\": \"face_images\"}},\n",
      "        {\"face_coordinates\": {\n",
      "          \"node_name\": \"extract_node\",\n",
      "          \"data_item\": \"face_coordinates\"}},\n",
      "        {\"confidence_levels\": {\n",
      "          \"node_name\": \"extract_node\",\n",
      "          \"data_item\": \"confidence_levels\"}},\n",
      "        {\"ages\": {\n",
      "          \"node_name\": \"age_gender_recognition_node\",\n",
      "          \"data_item\": \"age\"}},\n",
      "        {\"genders\": {\n",
      "          \"node_name\": \"age_gender_recognition_node\",\n",
      "          \"data_item\": \"gender\"}},\n",
      "        {\"emotions\": {\n",
      "          \"node_name\": \"emotion_recognition_node\",\n",
      "          \"data_item\": \"emotion\"}}\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!wget -q https://raw.githubusercontent.com/dtrawins/openvino_notebooks/ovms-demo/notebooks/202-model-server/config.json\n",
    "!cat config.json    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b5ff3",
   "metadata": {},
   "source": [
    "Let's add the custom node library and the config.json to a ConfigMap resource. It will be mounted later inside the OpenVINO Model Server containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2d41a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/ovms-face-detection-pipeline created\n"
     ]
    }
   ],
   "source": [
    "!oc create configmap ovms-face-detection-pipeline --from-file=libcustom_node.so=libcustom_node.so --from-file=config.json=config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2baa0d",
   "metadata": {},
   "source": [
    "We are ready to deploy the model server using the following Ovms resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3963acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: intel.com/v1alpha1\n",
      "kind: Ovms\n",
      "metadata:\n",
      "  name: ovms-pipeline\n",
      "  namespace: ovms\n",
      "spec:\n",
      "  aws_access_key_id: \"minio\"\n",
      "  aws_region: \"us-east-1\"\n",
      "  aws_secret_access_key: \"minio123\"\n",
      "  s3_compat_api_endpoint: 'http://minio-service:9000'\n",
      "  config_configmap_name: 'ovms-face-detection-pipeline'\n",
      "  grpc_port: 8080\n",
      "  image_name: >-\n",
      "    registry.connect.redhat.com/intel/openvino-model-server:latest\n",
      "  log_level: INFO\n",
      "  plugin_config: '{\\\"CPU_THROUGHPUT_STREAMS\\\":\\\"1\\\"}'\n",
      "  replicas: 1\n",
      "  rest_port: 8081\n",
      "  service_type: ClusterIP\n"
     ]
    }
   ],
   "source": [
    "!curl -s https://raw.githubusercontent.com/dtrawins/openvino_notebooks/ovms-demo/notebooks/202-model-server/ovms-face-detection-pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "853e2cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ovms.intel.com/ovms-pipeline created\n"
     ]
    }
   ],
   "source": [
    "!oc apply -f https://raw.githubusercontent.com/dtrawins/openvino_notebooks/ovms-demo/notebooks/202-model-server/ovms-face-detection-pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "240f3151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\n",
      "minio-service   ClusterIP   172.30.157.153   <none>        9000/TCP            11m\n",
      "ovms-pipeline   ClusterIP   172.30.57.114    <none>        8080/TCP,8081/TCP   59s\n",
      "ovms-resnet     ClusterIP   172.30.88.10     <none>        8080/TCP,8081/TCP   7m2s\n",
      "NAME                             READY     STATUS    RESTARTS   AGE\n",
      "minio-5c57f888dd-69dzq           1/1       Running   0          11m\n",
      "ovms-pipeline-66494fbd96-2mhvz   1/1       Running   0          60s\n",
      "ovms-resnet-7cdb696f7b-pxl7z     1/1       Running   0          7m2s\n"
     ]
    }
   ],
   "source": [
    "!oc get service\n",
    "!oc get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db91b88",
   "metadata": {},
   "source": [
    "The model server with face detection pipeline is deployed we can test the status of models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77b6ba29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E0422 21:00:18.315934811    4824 backup_poller.cc:133]       Run client channel backup poller: {\"created\":\"@1619125218.315887516\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":321,\"referenced_errors\":[{\"created\":\"@1619125218.315843714\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":948,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n",
      "{\n",
      "\"age_gender_recognition\" : \n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"OK\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "},\n",
      "\"emotion_recognition\" : \n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"OK\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "},\n",
      "\"face_detection\" : \n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"OK\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "},\n",
      "\"find_face_images\" : \n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"OK\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n",
      "}"
     ]
    }
   ],
   "source": [
    "!curl -s http://ovms-pipeline.ovms.svc:8081/v1/config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5fbc9",
   "metadata": {},
   "source": [
    "The pipeline execution is represented as the model `find_face_images`. The client run the prediction request exectly the same way like with the single models. Below the gRPC client will perform remote inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e3151fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import make_tensor_proto, make_ndarray\n",
    "import argparse\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca0a50",
   "metadata": {},
   "source": [
    "Let's prepare the image to analyse\n",
    "![image](https://github.com/openvinotoolkit/model_server/raw/develop/example_client/images/people/people2.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c35b4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/openvinotoolkit/model_server/raw/develop/example_client/images/people/people2.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fba88a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 400, 600)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('people2.jpeg').astype(np.float32)  # BGR color format, shape HWC\n",
    "resolution = (400, 600)\n",
    "img = cv2.resize(img, (resolution[1], resolution[0]))\n",
    "img = img.transpose(2,0,1).reshape(1,3,resolution[0],resolution[1])\n",
    "print(img.shape)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83c866",
   "metadata": {},
   "source": [
    "Next we establish connection with the model server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b6cff27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"ovms-pipeline.ovms.svc:8080\"\n",
    "MAX_MESSAGE_LENGTH = 1024 * 1024 * 8  # incresed default max size of the message\n",
    "channel = grpc.insecure_channel(address,\n",
    "    options=[\n",
    "        ('grpc.max_send_message_length', MAX_MESSAGE_LENGTH),\n",
    "        ('grpc.max_receive_message_length', MAX_MESSAGE_LENGTH),\n",
    "    ])\n",
    "\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = \"find_face_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c62ca",
   "metadata": {},
   "source": [
    "Below is created the request and prediction gets executed. Note, that the exeption is handled when the pipeline don't detect any face in the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b54a46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "request.inputs['image'].CopyFrom(make_tensor_proto(img, shape=img.shape))\n",
    "try:\n",
    "    response = stub.Predict(request, 10.0)\n",
    "except grpc.RpcError as err:\n",
    "    if err.code() == grpc.StatusCode.ABORTED:\n",
    "        print('No face has been found in the image')\n",
    "        exit(1)\n",
    "    else:\n",
    "        raise err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20e747",
   "metadata": {},
   "source": [
    "Results are in the `response` object in a set of outputs: `ages`, `genders`, `emotions`, `face_coordinates`. Ouput `face_images` returns cropped faces retreived from the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d621fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_face_images_as_jpgs(output_nd, name, location):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        out = output_nd[i][0]\n",
    "        out = out.transpose(1,2,0)\n",
    "        cv2.imwrite(os.path.join(location, name + '_' + str(i) + '.jpg'), out)\n",
    "        \n",
    "def update_people_ages(output_nd, people):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        age = int(output_nd[i,0,0,0,0] * 100)\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'age': age})\n",
    "        else:\n",
    "            people[i].update({'age': age})\n",
    "    return people\n",
    "\n",
    "def update_people_genders(output_nd, people):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        gender = 'male' if output_nd[i,0,0,0,0] < output_nd[i,0,1,0,0] else 'female'\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'gender': gender})\n",
    "        else:\n",
    "            people[i].update({'gender': gender})\n",
    "    return people\n",
    "\n",
    "def update_people_emotions(output_nd, people):\n",
    "    emotion_names = {\n",
    "        0: 'neutral',\n",
    "        1: 'happy',\n",
    "        2: 'sad',\n",
    "        3: 'surprised',\n",
    "        4: 'angry'\n",
    "    }\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        emotion_id = np.argmax(output_nd[i,0,:,0,0])\n",
    "        emotion = emotion_names[emotion_id]\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'emotion': emotion})\n",
    "        else:\n",
    "            people[i].update({'emotion': emotion})\n",
    "    return people\n",
    "\n",
    "def update_people_coordinate(output_nd, people):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'coordinate': output_nd[i,0,:]})\n",
    "        else:\n",
    "            people[i].update({'coordinate': output_nd[i,0,:]})\n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "dd9b8073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: name[face_coordinates]\n",
      "    numpy => shape[(2, 1, 4)] data[float32]\n",
      "Output: name[genders]\n",
      "    numpy => shape[(2, 1, 2, 1, 1)] data[float32]\n",
      "Output: name[face_images]\n",
      "    numpy => shape[(2, 1, 3, 64, 64)] data[float32]\n",
      "Output: name[confidence_levels]\n",
      "    numpy => shape[(2, 1, 1)] data[float32]\n",
      "Output: name[ages]\n",
      "    numpy => shape[(2, 1, 1, 1, 1)] data[float32]\n",
      "Output: name[emotions]\n",
      "    numpy => shape[(2, 1, 5, 1, 1)] data[float32]\n"
     ]
    }
   ],
   "source": [
    "people = []\n",
    "\n",
    "for name in response.outputs:\n",
    "    print(f\"Output: name[{name}]\")\n",
    "    tensor_proto = response.outputs[name]\n",
    "    output_nd = make_ndarray(tensor_proto)\n",
    "    print(f\"    numpy => shape[{output_nd.shape}] data[{output_nd.dtype}]\")\n",
    "\n",
    "    if name == 'face_images':\n",
    "        save_face_images_as_jpgs(output_nd, name, \".\")\n",
    "    if name == 'ages':\n",
    "        people = update_people_ages(output_nd, people)\n",
    "    if name == 'genders':\n",
    "        people = update_people_genders(output_nd, people)\n",
    "    if name == 'emotions':\n",
    "        people = update_people_emotions(output_nd, people)\n",
    "    if name == 'face_coordinates':\n",
    "        people = update_people_coordinate(output_nd, people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "26010213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 2 faces:\n",
      "Age: 48 ; Gender: male ; Emotion: happy ; Original image coordinate: [0.4584167  0.11273344 0.6308887  0.4844094 ]\n",
      "Age: 20 ; Gender: female ; Emotion: happy ; Original image coordinate: [0.11928874 0.5223113  0.25304735 0.75718385]\n"
     ]
    }
   ],
   "source": [
    "print('\\nFound', len(people), 'faces:')\n",
    "for person in people:\n",
    "    print('Age:', person['age'], '; Gender:', person['gender'], '; Emotion:', person['emotion'], '; Original image coordinate:', person['coordinate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c9e92",
   "metadata": {},
   "source": [
    "![person1](face_images_0.jpg)\n",
    "![person2](face_images_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e3dcf",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "23225d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf emotions\n",
    "!rm -Rf age-gender\n",
    "!rm -Rf face-detection\n",
    "!rm -Rf resnet\n",
    "!rm -Rf model_server\n",
    "!rm -Rf opencv\n",
    "!rm bee.jpeg classes.py config.json mc people* face_images* model_zoo_intel_object_detection* libcustom_node.so\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc delete Ovms ovms-resnet\n",
    "!oc delete Ovms ovms-pipeline\n",
    "!oc delete deploy minio\n",
    "!oc delete service minio-service\n",
    "!oc delete configmap ovms-face-detection-pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
